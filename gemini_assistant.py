# -*- coding: utf-8 -*-
"""ToDo Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fcidF56UAv_WAyHgDt2oTs6xa6CXIW_p
"""

!pip install --quiet langgraph langchain_google_genai google-ai-generativelanguage==0.6.15

import os,getpass

GEMINI_API_KEY=getpass.getpass("Enter Gemini API key")
os.environ["GEMINI_API_KEY"]=GEMINI_API_KEY

from langchain_google_genai import ChatGoogleGenerativeAI

llm=ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0,google_api_key=GEMINI_API_KEY)

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.store.base import BaseStore

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables.config import RunnableConfig
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver

MODEL_SYSTEM_MESSAGE = """You are a helpful assistant with memory that provides information about the user.
If you have memory for this user, use it to personalize your responses.
Here is the memory (it may be empty): {memory}"""

# Create new memory from the chat history and any existing memory
CREATE_MEMORY_INSTRUCTION = """"You are collecting information about the user to personalize your responses.

CURRENT USER INFORMATION:
{memory}

INSTRUCTIONS:
1. Review the chat history below carefully
2. Identify new information about the user, such as:
   - Personal details (name, location)
   - Preferences (likes, dislikes)
   - Interests and hobbies
   - Past experiences
   - Goals or future plans
3. Merge any new information with existing memory
4. Format the memory as a clear, bulleted list
5. If new information conflicts with existing memory, keep the most recent version

Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.

Based on the chat history below, please update the user information:"""

def call_model(state:MessagesState, config:RunnableConfig, store:BaseStore):
  #getting user_id from config
  user_id=config["configurable"]["user_id"]

  #retreiving memory from store
  namespace=("memory",user_id)
  key="user_memory"
  existing_memory=store.get(namespace, key)

  if existing_memory:
    #getting memory content from value
    existing_memory_value=existing_memory.value.get("memory")
  else:
    existing_memory_value="No exisiting memory found"

  formatted_memory=MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_value)
  messages=[SystemMessage(content=formatted_memory)]+state["messages"]

  response=llm.invoke(messages)

  return {"messages":response}

def write_model(state:MessagesState, config:RunnableConfig, store:BaseStore):

  #get user_id from config
  user_id=config["configurable"]["user_id"]

  #retrieve existing memory
  namespace=("memory",user_id)
  existing_memory=store.get(namespace, "user_memory")

  if existing_memory:
    #getting memory content from value
    existing_memory_value=existing_memory.value.get("memory")
  else:
    existing_memory_value="No exisiting memory found"

  #format the memory
  formatted_memory=CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_value)
  messages=[SystemMessage(content=formatted_memory)]+state["messages"]

  new_memory = llm.invoke(messages)
  key="user_memory"

  store.put(namespace, key, {"memory":new_memory.content})

  #return {"messages":state["messages"]+new_memory}

workflow=StateGraph(MessagesState)
workflow.add_node("call_model", call_model)
workflow.add_node("write_model", write_model)

workflow.add_edge(START, "call_model")
workflow.add_edge("call_model", "write_model")
workflow.add_edge("write_model", END)

across_thread_memory=InMemoryStore()
within_thread_memory=MemorySaver()

graph=workflow.compile(checkpointer=within_thread_memory, store=across_thread_memory)

config={"configurable":{"user_id":"1","thread_id":"1"}}

input_message=[HumanMessage(content="Hi. I'm Monisha")]
for chunk in graph.stream({"messages":input_message}, config, stream_mode="values"):
  chunk["messages"][-1].pretty_print()

input_message=[HumanMessage(content="I'm planning for a trip in Thailand")]
for chunk in graph.stream({
    "messages":input_message}, config, stream_mode="values"):
  chunk["messages"][-1].pretty_print()

thread = {"configurable": {"thread_id": "1"}}
state = graph.get_state(thread).values
for m in state["messages"]:
    m.pretty_print()

# We supply a user ID for across-thread memory as well as a new thread ID
config={"configurable":{"user_id":"1","thread_id":"2"}}

# User input
input_messages = [HumanMessage(content="Hi. Can you tell my name")]

# Run the graph
for chunk in graph.stream({"messages": input_messages}, config, stream_mode="values"):
    chunk["messages"][-1].pretty_print()